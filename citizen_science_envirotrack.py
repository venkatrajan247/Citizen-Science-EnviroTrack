# -*- coding: utf-8 -*-
"""Citizen-Science-EnviroTrack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKbKKBZt3AubRTdZ3kRlvbWdvPah6XaD
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pip3-autoremove
# !pip-autoremove torch torchvision torchaudio -y
# !pip install "torch==2.4.0" "xformers==0.0.27.post2" triton torchvision torchaudio
# !pip install "unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git"
#

!pip install "unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git"

import os
os.environ["WANDB_DISABLED"] = "true"

import torch
from unsloth import PatchDPOTrainer
from transformers import TrainingArguments
from trl import DPOTrainer
from unsloth import FastLanguageModel
from datasets import load_dataset
from unsloth import FastLanguageModel  # âœ… Import FastLanguageModel

PatchDPOTrainer()

# Load the dataset from the specified Hugging Face repository and split it into the 'train' set
dataset = load_dataset("Venkatrajan247/citizen-science-monitoring", split='train')

# Rename the 'text' column to 'chosen' for clarity and to align with the expected column naming
dataset = dataset.rename_column('text', 'chosen')

# Rename the 'rejected_text' column to 'rejected' for consistency with the 'chosen' column
dataset = dataset.rename_column('rejected_text', 'rejected')

# print(dataset)
print(dataset)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Define the maximum sequence length for the model to process
max_seq_length = 4096

# Specify the data type for the model (set to None for default behavior)
dtype = None

# Enable 4-bit quantization to reduce memory usage; set to False if higher precision is required
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    attn_implementation="flash_attention_2"  # Faster training & inference
)

# Apply PEFT (Parameter-Efficient Fine-Tuning) to the quantized model using LoRA
model = FastLanguageModel.get_peft_model(
    model,  # The pre-trained and potentially quantized model

    # The rank of the low-rank decomposition matrices for LoRA. Higher values allow
    # the model to capture more task-specific information (common values: 8, 16, 32, 64, 128).
    r=64,

    # Specify the target modules where LoRA should be applied, such as key, value,
    # query projections, and other attention-related layers.
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention projection layers
        "gate_proj", "up_proj", "down_proj",    # Feed-forward network layers
    ],

    # LoRA-specific hyperparameters:
    lora_alpha=64,  # Scaling factor for the LoRA update
    lora_dropout=0,  # Dropout for LoRA layers; set to 0 for no dropout
    bias="none",  # Bias handling strategy; options are "none", "all", or "lora_only"

    # Enable gradient checkpointing for memory efficiency during training.
    # "unsloth" is a custom value, likely to support longer context lengths.
    use_gradient_checkpointing="unsloth",

    random_state=3407,  # Random seed for reproducibility during training

    # Optional configurations for advanced techniques:
    use_rslora=False,  # Flag for whether to use Randomized Singular LoRA (RsLoRA)
    loftq_config=None,  # Configuration for LoFT-Q (LoRA for Fine-Tuning Quantized models)
)

!pip show unsloth

# Initialize the DPOTrainer (Direct Preference Optimization Trainer) to fine-tune the model
dpo_trainer = DPOTrainer(
    model=model,  # The model to be fine-tuned (already loaded and potentially quantized with LoRA)

    # Define training arguments for fine-tuning
    args=TrainingArguments(
        per_device_train_batch_size=2,  # Number of samples processed per device in one forward/backward pass
        gradient_accumulation_steps=3,  # Accumulate gradients over multiple steps to simulate a larger batch size
        warmup_ratio=0.1,  # Fraction of total steps used for learning rate warmup
        num_train_epochs=1,  # Number of training epochs
        learning_rate=5e-6,  # Initial learning rate for the optimizer
        fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 if BF16 is not supported
        bf16=torch.cuda.is_bf16_supported(),  # Use BF16 precision if supported by the GPU
        logging_steps=10,  # Log training progress every 10 steps
        optim="adamw_8bit",  # Use AdamW optimizer with 8-bit precision for memory efficiency
        weight_decay=0.001,  # Weight decay for regularization
        lr_scheduler_type="linear",  # Use a linear learning rate scheduler
        seed=42,  # Set random seed for reproducibility
        report_to="none",  # Disable reporting to external tools (e.g., WandB or TensorBoard)
        output_dir="outputs",  # Directory to save the model and training artifacts
    ),

    beta=0.1,  # Beta parameter for DPO; balances the loss for optimization
    train_dataset=dataset["train"],  # Training dataset
    eval_dataset=dataset["test"],  # Evaluation dataset
    tokenizer=tokenizer,  # Tokenizer corresponding to the model
    max_length=4096,  # Maximum token length for input sequences
    max_prompt_length=512,  # Maximum token length for prompts within input sequences
)

dpo_trainer.train()

from huggingface_hub import create_repo

repo_name = "citizen-science-monitoring-dpo-trained"
username = "Venkatrajan247"  # Replace with your HF username

create_repo(f"{username}/{repo_name}", repo_type="model", exist_ok=True)

model.save_pretrained("citizen-science-monitoring-dpo-trained")
tokenizer.save_pretrained("citizen-science-monitoring-dpo-trained")

from huggingface_hub import notebook_login

notebook_login()

from transformers import AutoTokenizer

# Define your Hugging Face repo name
repo_name = "Venkatrajan247/citizen-science-monitoring-dpo-trained"  # Update with your username

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("./citizen-science-monitoring-dpo-trained")

# Push to Hugging Face Hub
tokenizer.push_to_hub(repo_name)

model.push_to_hub(repo_name)

from unsloth import FastLanguageModel
import transformers

# Load the fine-tuned model and tokenizer from the specified directory
# The model is loaded with 4-bit quantization enabled for memory efficiency
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="citizen-science-monitoring-dpo-trained",  # Path to the saved fine-tuned model
    max_seq_length=4096,  # Set the maximum sequence length for processing inputs
    load_in_4bit=True,  # Enable 4-bit quantization to reduce memory usage
)

# Prepare the model for inference (set it to evaluation mode)
FastLanguageModel.for_inference(model)  # This configures the model for inference without further training

def predict(input_prompt):
    """
    Generate a response based on the input prompt using a pre-trained language model.

    Parameters:
        input_prompt (str): The user's query or instruction.

    Returns:
        str: The model-generated response.
    """

    # Ensure model and tokenizer are on the correct device (GPU if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Define conversation context
    messages = [
        {"role": "system", "content": "You are a helpful assistant that provides security recommendations for organizations."},
        {"role": "user", "content": input_prompt}
    ]

    # Convert messages into the model's chat format
    formatted_input = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    # Tokenize input and move it to the correct device
    model_inputs = tokenizer([formatted_input], return_tensors="pt").to(device)

    # Generate response with controlled parameters
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,  # Reduced from 4096 for efficiency
        temperature=0.2  # Controls randomness; lower = more deterministic
    )

    # Remove input tokens from output (to get only generated response)
    response_ids = [
        output[len(input_seq):] for input_seq, output in zip(model_inputs.input_ids, generated_ids)
    ]

    # Decode response into human-readable text
    response = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]

    return response

from IPython.display import display, Markdown

def format_response(text):
    display(Markdown(f"**Response:**\n\n{text}"))

input_prompt = "How should invasive species be mapped?"
response = predict(input_prompt)
format_response(response)

from IPython.display import display, Markdown

def format_response(text):
    display(Markdown(f"**Response:**\n\n{text}"))

input_prompt = "'Can you summarize your response to \"How should invasive species be mapped?\"in one sentence?'"
response = predict(input_prompt)
format_response(response)

from IPython.display import display, Markdown

def format_response(text):
    display(Markdown(f"**Response:**\n\n{text}"))

input_prompt = "what are the seven wonders in world and explain their interesting thing"
response = predict(input_prompt)
format_response(response)

!pip install streamlit transformers torch

from huggingface_hub import upload_file

upload_file(
    path_or_fileobj="citizen-science-monitoring-dpo-trained/tokenizer.model",
    path_in_repo="tokenizer.model",
    repo_id="Venkatrajan247/citizen-science-monitoring-dpo-trained",
    repo_type="model"
)

from transformers import AutoTokenizer

model_name = "Venkatrajan247/citizen-science-monitoring-dpo-trained"

tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True, use_fast=False)

